{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 64, "column": 0}, "map": {"version":3,"sources":["file:///D:/search-engine-spec/lib/db.ts"],"sourcesContent":["import { Pool } from '@neondatabase/serverless';\r\nimport * as dotenv from 'dotenv';\r\n\r\ndotenv.config({ path: '.env.local' });\r\ndotenv.config({ path: '.env' });\r\n\r\nconsole.log('Initializing Neon DB pool...');\r\n\r\nconst pool = new Pool({\r\n    connectionString: process.env.DATABASE_URL || 'postgresql://postgres:postgres@localhost:5432/search_engine',\r\n    ssl: true,\r\n});\r\n\r\nexport default pool;\r\n"],"names":[],"mappings":";;;;AAAA;AACA;;;AAEA,iJAAa,CAAC;IAAE,MAAM;AAAa;AACnC,iJAAa,CAAC;IAAE,MAAM;AAAO;AAE7B,QAAQ,GAAG,CAAC;AAEZ,MAAM,OAAO,IAAI,gKAAI,CAAC;IAClB,kBAAkB,QAAQ,GAAG,CAAC,YAAY,IAAI;IAC9C,KAAK;AACT;uCAEe"}},
    {"offset": {"line": 88, "column": 0}, "map": {"version":3,"sources":["file:///D:/search-engine-spec/lib/services/search-service.ts"],"sourcesContent":["import pool from '../db';\r\n\r\nexport interface SearchParams {\r\n    query: string;\r\n    page?: number;\r\n    pageSize?: number;\r\n    filters?: {\r\n        language?: string;\r\n        dateRange?: {\r\n            start?: string;\r\n            end?: string;\r\n        };\r\n        site?: string | string[];\r\n    };\r\n}\r\n\r\nexport interface SearchResult {\r\n    doc_id: string;\r\n    url: string;\r\n    title: string;\r\n    snippet: string;\r\n    score: number;\r\n    highlights: string[];\r\n    metadata: {\r\n        author?: string;\r\n        published_date?: string;\r\n        language?: string;\r\n    };\r\n}\r\n\r\nexport interface SearchResponse {\r\n    query: {\r\n        original: string;\r\n        corrected?: string;\r\n        intent?: string;\r\n    };\r\n    results: SearchResult[];\r\n    total_results: number;\r\n    page: number;\r\n    page_size: number;\r\n    query_time_ms: number;\r\n    suggestions: string[];\r\n}\r\n\r\nexport class SearchService {\r\n    async search(params: SearchParams): Promise<SearchResponse> {\r\n        const startTime = Date.now();\r\n        const { query, page = 1, pageSize = 10 } = params;\r\n        const offset = (page - 1) * pageSize;\r\n\r\n        // Handle single character/variable searches with pattern matching\r\n        const isSingleChar = query.trim().length === 1;\r\n        const siteFilter = params.filters?.site;\r\n        const hasSiteFilter = siteFilter && (Array.isArray(siteFilter) ? siteFilter.length > 0 : true);\r\n        const siteCondition = hasSiteFilter\r\n            ? Array.isArray(siteFilter)\r\n                ? `AND url ILIKE ANY($${isSingleChar ? 5 : 4})`\r\n                : `AND url ILIKE $${isSingleChar ? 5 : 4}`\r\n            : '';\r\n\r\n        let sql;\r\n        let queryParams: any[];\r\n\r\n        if (isSingleChar) {\r\n            // For single character searches, use pattern matching instead of full-text search\r\n            const searchPattern = `%${query.trim()}%`;\r\n            sql = `\r\n                WITH ranked_docs AS (\r\n                    SELECT \r\n                        doc_id,\r\n                        url,\r\n                        title,\r\n                        meta_description,\r\n                        body,\r\n                        pagerank,\r\n                        crawl_timestamp,\r\n                        language,\r\n                        CASE \r\n                            WHEN title ILIKE $1 THEN 3.0\r\n                            WHEN meta_description ILIKE $1 THEN 2.0\r\n                            WHEN body ILIKE $1 THEN 1.0\r\n                            ELSE 0.5\r\n                        END as rank\r\n                    FROM documents\r\n                    WHERE \r\n                        (title ILIKE $1 OR \r\n                         meta_description ILIKE $1 OR \r\n                         body ILIKE $1)\r\n                        ${siteCondition}\r\n                ),\r\n                total_count AS (\r\n                    SELECT COUNT(*) as count FROM ranked_docs\r\n                )\r\n                SELECT \r\n                    rd.*,\r\n                    tc.count as total_results,\r\n                    CASE \r\n                        WHEN title ILIKE $1 THEN regexp_replace(title, '(' || $2 || ')', '<em>\\\\1</em>', 'gi')\r\n                        ELSE title\r\n                    END as title_highlight,\r\n                    CASE \r\n                        WHEN body ILIKE $1 THEN regexp_replace(substring(body, 1, 500), '(' || $2 || ')', '<em>\\\\1</em>', 'gi')\r\n                        ELSE substring(body, 1, 500)\r\n                    END as body_highlight\r\n                FROM ranked_docs rd, total_count tc\r\n                ORDER BY rank DESC, pagerank DESC\r\n                LIMIT $3 OFFSET $4\r\n            `;\r\n            queryParams = [searchPattern, query.trim(), pageSize, offset];\r\n            if (hasSiteFilter) {\r\n                const patterns = Array.isArray(siteFilter) ? siteFilter.map(s => `%${s}%`) : `%${siteFilter}%`;\r\n                queryParams.push(patterns);\r\n            }\r\n        } else {\r\n            // PostgreSQL Full-Text Search for longer queries\r\n            sql = `\r\n                WITH search_query AS (\r\n                    SELECT websearch_to_tsquery('english', $1) as query\r\n                ),\r\n                ranked_docs AS (\r\n                    SELECT \r\n                        doc_id,\r\n                        url,\r\n                        title,\r\n                        meta_description,\r\n                        body,\r\n                        pagerank,\r\n                        crawl_timestamp,\r\n                        language,\r\n                        ts_rank_cd(\r\n                            setweight(to_tsvector('english', title), 'A') ||\r\n                            setweight(to_tsvector('english', coalesce(meta_description, '')), 'B') ||\r\n                            setweight(to_tsvector('english', coalesce(body, '')), 'C'),\r\n                            (SELECT query FROM search_query)\r\n                        ) as rank\r\n                    FROM documents\r\n                    WHERE \r\n                        (\r\n                            setweight(to_tsvector('english', title), 'A') ||\r\n                            setweight(to_tsvector('english', coalesce(meta_description, '')), 'B') ||\r\n                            setweight(to_tsvector('english', coalesce(body, '')), 'C')\r\n                        ) @@ (SELECT query FROM search_query)\r\n                        ${siteCondition}\r\n                ),\r\n                total_count AS (\r\n                    SELECT COUNT(*) as count FROM ranked_docs\r\n                )\r\n                SELECT \r\n                    rd.*,\r\n                    tc.count as total_results,\r\n                    ts_headline('english', rd.body, (SELECT query FROM search_query), \r\n                        'StartSel=<em>, StopSel=</em>, MaxWords=35, MinWords=15, ShortWord=3, HighlightAll=FALSE, MaxFragments=1, FragmentDelimiter=\"...\"'\r\n                    ) as body_highlight,\r\n                    ts_headline('english', rd.title, (SELECT query FROM search_query), \r\n                        'StartSel=<em>, StopSel=</em>, HighlightAll=FALSE'\r\n                    ) as title_highlight\r\n                FROM ranked_docs rd, total_count tc\r\n                ORDER BY rank DESC, pagerank DESC\r\n                LIMIT $2 OFFSET $3\r\n            `;\r\n            queryParams = [query, pageSize, offset];\r\n            if (hasSiteFilter) {\r\n                const patterns = Array.isArray(siteFilter) ? siteFilter.map(s => `%${s}%`) : `%${siteFilter}%`;\r\n                queryParams.push(patterns);\r\n            }\r\n        }\r\n\r\n        try {\r\n            console.log(`Executing ${isSingleChar ? 'single character' : 'full-text'} search for query: \"${query}\"`);\r\n            const result = await pool.query(sql, queryParams);\r\n            console.log(`SQL executed. Rows: ${result.rows.length}`);\r\n\r\n            const totalResults = result.rows.length > 0 ? parseInt(result.rows[0].total_results, 10) : 0;\r\n\r\n            const searchResults: SearchResult[] = result.rows.map((row: any) => ({\r\n                doc_id: row.doc_id,\r\n                url: row.url,\r\n                title: row.title,\r\n                snippet: isSingleChar\r\n                    ? (row.body_highlight || row.meta_description || row.body || '').substring(0, 200) + '...'\r\n                    : (row.body_highlight || row.meta_description || ''),\r\n                score: parseFloat(row.rank) || 0,\r\n                highlights: [\r\n                    row.title_highlight,\r\n                    row.body_highlight\r\n                ].filter(Boolean),\r\n                metadata: {\r\n                    language: row.language,\r\n                    published_date: row.crawl_timestamp,\r\n                },\r\n            }));\r\n\r\n            const endTime = Date.now();\r\n\r\n            // Log the search event\r\n            this.logSearchEvent(query, totalResults, endTime - startTime);\r\n\r\n            return {\r\n                query: {\r\n                    original: query,\r\n                },\r\n                results: searchResults,\r\n                total_results: totalResults,\r\n                page,\r\n                page_size: pageSize,\r\n                query_time_ms: endTime - startTime,\r\n                suggestions: [],\r\n            };\r\n        } catch (error) {\r\n            console.error('Search error:', error);\r\n            throw error;\r\n        }\r\n    }\r\n\r\n    private async logSearchEvent(query: string, resultCount: number, executionTime: number) {\r\n        try {\r\n            const sql = `\r\n                INSERT INTO search_events (query_text, result_count, execution_time_ms)\r\n                VALUES ($1, $2, $3)\r\n            `;\r\n            await pool.query(sql, [query, resultCount, executionTime]);\r\n        } catch (err) {\r\n            console.error('Failed to log search event:', err);\r\n        }\r\n    }\r\n\r\n    async suggest(query: string, limit: number = 10): Promise<string[]> {\r\n        const sql = `\r\n            SELECT query_text \r\n            FROM query_suggestions \r\n            WHERE query_text ILIKE $1 \r\n            ORDER BY frequency DESC \r\n            LIMIT $2\r\n        `;\r\n        try {\r\n            const result = await pool.query(sql, [`${query}%`, limit]);\r\n            return result.rows.map((row: any) => row.query_text);\r\n        } catch (error) {\r\n            console.error('Suggestion error:', error);\r\n            return [];\r\n        }\r\n    }\r\n}\r\n\r\nexport const searchService = new SearchService();\r\n"],"names":[],"mappings":";;;;;;AAAA;;AA4CO,MAAM;IACT,MAAM,OAAO,MAAoB,EAA2B;QACxD,MAAM,YAAY,KAAK,GAAG;QAC1B,MAAM,EAAE,KAAK,EAAE,OAAO,CAAC,EAAE,WAAW,EAAE,EAAE,GAAG;QAC3C,MAAM,SAAS,CAAC,OAAO,CAAC,IAAI;QAE5B,kEAAkE;QAClE,MAAM,eAAe,MAAM,IAAI,GAAG,MAAM,KAAK;QAC7C,MAAM,aAAa,OAAO,OAAO,EAAE;QACnC,MAAM,gBAAgB,cAAc,CAAC,MAAM,OAAO,CAAC,cAAc,WAAW,MAAM,GAAG,IAAI,IAAI;QAC7F,MAAM,gBAAgB,gBAChB,MAAM,OAAO,CAAC,cACV,CAAC,mBAAmB,EAAE,eAAe,IAAI,EAAE,CAAC,CAAC,GAC7C,CAAC,eAAe,EAAE,eAAe,IAAI,GAAG,GAC5C;QAEN,IAAI;QACJ,IAAI;QAEJ,IAAI,cAAc;YACd,kFAAkF;YAClF,MAAM,gBAAgB,CAAC,CAAC,EAAE,MAAM,IAAI,GAAG,CAAC,CAAC;YACzC,MAAM,CAAC;;;;;;;;;;;;;;;;;;;;;;wBAsBK,EAAE,cAAc;;;;;;;;;;;;;;;;;;;YAmB5B,CAAC;YACD,cAAc;gBAAC;gBAAe,MAAM,IAAI;gBAAI;gBAAU;aAAO;YAC7D,IAAI,eAAe;gBACf,MAAM,WAAW,MAAM,OAAO,CAAC,cAAc,WAAW,GAAG,CAAC,CAAA,IAAK,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC,EAAE,WAAW,CAAC,CAAC;gBAC9F,YAAY,IAAI,CAAC;YACrB;QACJ,OAAO;YACH,iDAAiD;YACjD,MAAM,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;wBA2BK,EAAE,cAAc;;;;;;;;;;;;;;;;;YAiB5B,CAAC;YACD,cAAc;gBAAC;gBAAO;gBAAU;aAAO;YACvC,IAAI,eAAe;gBACf,MAAM,WAAW,MAAM,OAAO,CAAC,cAAc,WAAW,GAAG,CAAC,CAAA,IAAK,CAAC,CAAC,EAAE,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC,EAAE,WAAW,CAAC,CAAC;gBAC9F,YAAY,IAAI,CAAC;YACrB;QACJ;QAEA,IAAI;YACA,QAAQ,GAAG,CAAC,CAAC,UAAU,EAAE,eAAe,qBAAqB,YAAY,oBAAoB,EAAE,MAAM,CAAC,CAAC;YACvG,MAAM,SAAS,MAAM,sHAAI,CAAC,KAAK,CAAC,KAAK;YACrC,QAAQ,GAAG,CAAC,CAAC,oBAAoB,EAAE,OAAO,IAAI,CAAC,MAAM,EAAE;YAEvD,MAAM,eAAe,OAAO,IAAI,CAAC,MAAM,GAAG,IAAI,SAAS,OAAO,IAAI,CAAC,EAAE,CAAC,aAAa,EAAE,MAAM;YAE3F,MAAM,gBAAgC,OAAO,IAAI,CAAC,GAAG,CAAC,CAAC,MAAa,CAAC;oBACjE,QAAQ,IAAI,MAAM;oBAClB,KAAK,IAAI,GAAG;oBACZ,OAAO,IAAI,KAAK;oBAChB,SAAS,eACH,CAAC,IAAI,cAAc,IAAI,IAAI,gBAAgB,IAAI,IAAI,IAAI,IAAI,EAAE,EAAE,SAAS,CAAC,GAAG,OAAO,QAClF,IAAI,cAAc,IAAI,IAAI,gBAAgB,IAAI;oBACrD,OAAO,WAAW,IAAI,IAAI,KAAK;oBAC/B,YAAY;wBACR,IAAI,eAAe;wBACnB,IAAI,cAAc;qBACrB,CAAC,MAAM,CAAC;oBACT,UAAU;wBACN,UAAU,IAAI,QAAQ;wBACtB,gBAAgB,IAAI,eAAe;oBACvC;gBACJ,CAAC;YAED,MAAM,UAAU,KAAK,GAAG;YAExB,uBAAuB;YACvB,IAAI,CAAC,cAAc,CAAC,OAAO,cAAc,UAAU;YAEnD,OAAO;gBACH,OAAO;oBACH,UAAU;gBACd;gBACA,SAAS;gBACT,eAAe;gBACf;gBACA,WAAW;gBACX,eAAe,UAAU;gBACzB,aAAa,EAAE;YACnB;QACJ,EAAE,OAAO,OAAO;YACZ,QAAQ,KAAK,CAAC,iBAAiB;YAC/B,MAAM;QACV;IACJ;IAEA,MAAc,eAAe,KAAa,EAAE,WAAmB,EAAE,aAAqB,EAAE;QACpF,IAAI;YACA,MAAM,MAAM,CAAC;;;YAGb,CAAC;YACD,MAAM,sHAAI,CAAC,KAAK,CAAC,KAAK;gBAAC;gBAAO;gBAAa;aAAc;QAC7D,EAAE,OAAO,KAAK;YACV,QAAQ,KAAK,CAAC,+BAA+B;QACjD;IACJ;IAEA,MAAM,QAAQ,KAAa,EAAE,QAAgB,EAAE,EAAqB;QAChE,MAAM,MAAM,CAAC;;;;;;QAMb,CAAC;QACD,IAAI;YACA,MAAM,SAAS,MAAM,sHAAI,CAAC,KAAK,CAAC,KAAK;gBAAC,GAAG,MAAM,CAAC,CAAC;gBAAE;aAAM;YACzD,OAAO,OAAO,IAAI,CAAC,GAAG,CAAC,CAAC,MAAa,IAAI,UAAU;QACvD,EAAE,OAAO,OAAO;YACZ,QAAQ,KAAK,CAAC,qBAAqB;YACnC,OAAO,EAAE;QACb;IACJ;AACJ;AAEO,MAAM,gBAAgB,IAAI"}},
    {"offset": {"line": 299, "column": 0}, "map": {"version":3,"sources":["file:///D:/search-engine-spec/lib/services/cache-service.ts"],"sourcesContent":["// Local Cache Service (Free Tier)\r\nclass CacheService {\r\n    private localCache: Map<string, { value: any; expiry: number }>;\r\n    private readonly LOCAL_CACHE_TTL = 60 * 1000; // 1 minute\r\n\r\n    constructor() {\r\n        this.localCache = new Map();\r\n        console.log('CacheService initialized (Local Map)');\r\n\r\n        // Cleanup expired local cache entries\r\n        setInterval(() => {\r\n            const now = Date.now();\r\n            for (const [key, entry] of this.localCache.entries()) {\r\n                if (entry.expiry < now) {\r\n                    this.localCache.delete(key);\r\n                }\r\n            }\r\n        }, 60000); // Every minute\r\n    }\r\n\r\n    private getCacheKey(prefix: string, key: string): string {\r\n        return `${prefix}:${key}`;\r\n    }\r\n\r\n    async get<T>(prefix: string, key: string): Promise<T | null> {\r\n        const cacheKey = this.getCacheKey(prefix, key);\r\n        const localEntry = this.localCache.get(cacheKey);\r\n\r\n        if (localEntry && localEntry.expiry > Date.now()) {\r\n            return localEntry.value as T;\r\n        }\r\n\r\n        return null;\r\n    }\r\n\r\n    async set(prefix: string, key: string, value: any, ttl?: number): Promise<void> {\r\n        const cacheKey = this.getCacheKey(prefix, key);\r\n        this.localCache.set(cacheKey, {\r\n            value,\r\n            expiry: Date.now() + (ttl ? ttl * 1000 : this.LOCAL_CACHE_TTL),\r\n        });\r\n    }\r\n\r\n    async delete(prefix: string, key: string): Promise<void> {\r\n        const cacheKey = this.getCacheKey(prefix, key);\r\n        this.localCache.delete(cacheKey);\r\n    }\r\n\r\n    async invalidatePattern(pattern: string): Promise<void> {\r\n        const regex = new RegExp(pattern);\r\n        for (const key of this.localCache.keys()) {\r\n            if (regex.test(key)) {\r\n                this.localCache.delete(key);\r\n            }\r\n        }\r\n    }\r\n\r\n    async checkRateLimit(key: string, limit: number, window: number): Promise<boolean> {\r\n        // Simple memory-based rate limiting could be added here if needed\r\n        return true;\r\n    }\r\n\r\n    async disconnect(): Promise<void> {\r\n        // No-op for local map\r\n    }\r\n}\r\n\r\nexport const cacheService = new CacheService();\r\n"],"names":[],"mappings":"AAAA,kCAAkC;;;;;AAClC,MAAM;IACM,WAAwD;IAC/C,kBAAkB,KAAK,KAAK;IAE7C,aAAc;QACV,IAAI,CAAC,UAAU,GAAG,IAAI;QACtB,QAAQ,GAAG,CAAC;QAEZ,sCAAsC;QACtC,YAAY;YACR,MAAM,MAAM,KAAK,GAAG;YACpB,KAAK,MAAM,CAAC,KAAK,MAAM,IAAI,IAAI,CAAC,UAAU,CAAC,OAAO,GAAI;gBAClD,IAAI,MAAM,MAAM,GAAG,KAAK;oBACpB,IAAI,CAAC,UAAU,CAAC,MAAM,CAAC;gBAC3B;YACJ;QACJ,GAAG,QAAQ,eAAe;IAC9B;IAEQ,YAAY,MAAc,EAAE,GAAW,EAAU;QACrD,OAAO,GAAG,OAAO,CAAC,EAAE,KAAK;IAC7B;IAEA,MAAM,IAAO,MAAc,EAAE,GAAW,EAAqB;QACzD,MAAM,WAAW,IAAI,CAAC,WAAW,CAAC,QAAQ;QAC1C,MAAM,aAAa,IAAI,CAAC,UAAU,CAAC,GAAG,CAAC;QAEvC,IAAI,cAAc,WAAW,MAAM,GAAG,KAAK,GAAG,IAAI;YAC9C,OAAO,WAAW,KAAK;QAC3B;QAEA,OAAO;IACX;IAEA,MAAM,IAAI,MAAc,EAAE,GAAW,EAAE,KAAU,EAAE,GAAY,EAAiB;QAC5E,MAAM,WAAW,IAAI,CAAC,WAAW,CAAC,QAAQ;QAC1C,IAAI,CAAC,UAAU,CAAC,GAAG,CAAC,UAAU;YAC1B;YACA,QAAQ,KAAK,GAAG,KAAK,CAAC,MAAM,MAAM,OAAO,IAAI,CAAC,eAAe;QACjE;IACJ;IAEA,MAAM,OAAO,MAAc,EAAE,GAAW,EAAiB;QACrD,MAAM,WAAW,IAAI,CAAC,WAAW,CAAC,QAAQ;QAC1C,IAAI,CAAC,UAAU,CAAC,MAAM,CAAC;IAC3B;IAEA,MAAM,kBAAkB,OAAe,EAAiB;QACpD,MAAM,QAAQ,IAAI,OAAO;QACzB,KAAK,MAAM,OAAO,IAAI,CAAC,UAAU,CAAC,IAAI,GAAI;YACtC,IAAI,MAAM,IAAI,CAAC,MAAM;gBACjB,IAAI,CAAC,UAAU,CAAC,MAAM,CAAC;YAC3B;QACJ;IACJ;IAEA,MAAM,eAAe,GAAW,EAAE,KAAa,EAAE,MAAc,EAAoB;QAC/E,kEAAkE;QAClE,OAAO;IACX;IAEA,MAAM,aAA4B;IAC9B,sBAAsB;IAC1B;AACJ;AAEO,MAAM,eAAe,IAAI"}},
    {"offset": {"line": 363, "column": 0}, "map": {"version":3,"sources":["file:///D:/search-engine-spec/lib/services/hybrid-search-service.ts"],"sourcesContent":["import { SearchParams, SearchResponse, SearchResult, searchService } from './search-service';\r\nimport { cacheService } from './cache-service';\r\nimport crypto from 'crypto';\r\n\r\n/**\r\n * Hybrid Search Service\r\n * Orchestrates search across multiple backends with fallback and caching\r\n * Priority: Cache â†’ PostgreSQL (Free Tier)\r\n */\r\n\r\nexport interface HybridSearchOptions extends SearchParams {\r\n  useCache?: boolean;\r\n  preferElasticsearch?: boolean; // Ignored in free tier\r\n  timeout?: number;\r\n}\r\n\r\nexport class HybridSearchService {\r\n  private readonly CACHE_TTL = 300; // 5 minutes\r\n\r\n  /**\r\n   * Generate cache key for search query\r\n   */\r\n  private getCacheKey(params: SearchParams): string {\r\n    const normalized = {\r\n      query: params.query.toLowerCase().trim(),\r\n      page: params.page || 1,\r\n      pageSize: params.pageSize || 10,\r\n      filters: params.filters || {},\r\n    };\r\n    const hash = crypto.createHash('sha256').update(JSON.stringify(normalized)).digest('hex');\r\n    return `search:${hash}`;\r\n  }\r\n\r\n  /**\r\n   * Main search method\r\n   */\r\n  async search(params: HybridSearchOptions): Promise<SearchResponse> {\r\n    const startTime = Date.now();\r\n    const cacheKey = this.getCacheKey(params);\r\n    const useCache = params.useCache !== false;\r\n\r\n    try {\r\n      // Step 1: Try cache\r\n      if (useCache) {\r\n        const cached = await this.tryCache(cacheKey);\r\n        if (cached) {\r\n          console.log(`Cache hit for query: \"${params.query}\"`);\r\n          cached.query_time_ms = Date.now() - startTime;\r\n          return cached;\r\n        }\r\n      }\r\n\r\n      // Step 2: PostgreSQL (Source of Truth)\r\n      console.log('Using PostgreSQL for search (Free Tier)');\r\n      const pgResult = await searchService.search(params);\r\n\r\n      // Cache the result\r\n      if (useCache) {\r\n        await cacheService.set('search', cacheKey, pgResult, this.CACHE_TTL).catch((err) => {\r\n          console.error('Failed to cache PG result:', err);\r\n        });\r\n      }\r\n\r\n      pgResult.query_time_ms = Date.now() - startTime;\r\n      return pgResult;\r\n\r\n    } catch (error) {\r\n      console.error('Search failed:', error);\r\n      throw new Error('Search service temporarily unavailable');\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Try to get results from cache\r\n   */\r\n  private async tryCache(cacheKey: string): Promise<SearchResponse | null> {\r\n    try {\r\n      const cached = await cacheService.get<SearchResponse>('search', cacheKey);\r\n      return cached;\r\n    } catch (error) {\r\n      console.warn('Cache lookup failed:', error);\r\n      return null;\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Warm up cache with popular queries\r\n   */\r\n  async warmCache(queries: string[]): Promise<void> {\r\n    console.log(`Warming cache with ${queries.length} queries...`);\r\n    const promises = queries.map((query) =>\r\n      this.search({ query, page: 1, pageSize: 10, useCache: true }).catch((err) => {\r\n        console.error(`Failed to warm cache for query \"${query}\":`, err);\r\n      })\r\n    );\r\n    await Promise.all(promises);\r\n    console.log('Cache warming complete');\r\n  }\r\n\r\n  /**\r\n   * Invalidate cache for a query\r\n   */\r\n  async invalidateCache(params: SearchParams): Promise<void> {\r\n    const cacheKey = this.getCacheKey(params);\r\n    await cacheService.delete('search', cacheKey);\r\n  }\r\n\r\n  /**\r\n   * Get search backend health status\r\n   */\r\n  async getHealthStatus(): Promise<{\r\n    elasticsearch: boolean;\r\n    postgresql: boolean;\r\n    cache: boolean;\r\n  }> {\r\n    const pgHealth = await this.checkPostgresHealth().catch(() => false);\r\n\r\n    return {\r\n      elasticsearch: false, // Disabled\r\n      postgresql: pgHealth,\r\n      cache: true,\r\n    };\r\n  }\r\n\r\n  /**\r\n   * Check PostgreSQL health\r\n   */\r\n  private async checkPostgresHealth(): Promise<boolean> {\r\n    try {\r\n      // Try a simple search\r\n      await searchService.search({ query: 'test', page: 1, pageSize: 1 });\r\n      return true;\r\n    } catch {\r\n      return false;\r\n    }\r\n  }\r\n}\r\n\r\nexport const hybridSearchService = new HybridSearchService();\r\n\r\n"],"names":[],"mappings":";;;;;;AAAA;AACA;AACA;;;;AAcO,MAAM;IACM,YAAY,IAAI;IAEjC;;GAEC,GACD,AAAQ,YAAY,MAAoB,EAAU;QAChD,MAAM,aAAa;YACjB,OAAO,OAAO,KAAK,CAAC,WAAW,GAAG,IAAI;YACtC,MAAM,OAAO,IAAI,IAAI;YACrB,UAAU,OAAO,QAAQ,IAAI;YAC7B,SAAS,OAAO,OAAO,IAAI,CAAC;QAC9B;QACA,MAAM,OAAO,gHAAM,CAAC,UAAU,CAAC,UAAU,MAAM,CAAC,KAAK,SAAS,CAAC,aAAa,MAAM,CAAC;QACnF,OAAO,CAAC,OAAO,EAAE,MAAM;IACzB;IAEA;;GAEC,GACD,MAAM,OAAO,MAA2B,EAA2B;QACjE,MAAM,YAAY,KAAK,GAAG;QAC1B,MAAM,WAAW,IAAI,CAAC,WAAW,CAAC;QAClC,MAAM,WAAW,OAAO,QAAQ,KAAK;QAErC,IAAI;YACF,oBAAoB;YACpB,IAAI,UAAU;gBACZ,MAAM,SAAS,MAAM,IAAI,CAAC,QAAQ,CAAC;gBACnC,IAAI,QAAQ;oBACV,QAAQ,GAAG,CAAC,CAAC,sBAAsB,EAAE,OAAO,KAAK,CAAC,CAAC,CAAC;oBACpD,OAAO,aAAa,GAAG,KAAK,GAAG,KAAK;oBACpC,OAAO;gBACT;YACF;YAEA,uCAAuC;YACvC,QAAQ,GAAG,CAAC;YACZ,MAAM,WAAW,MAAM,uJAAa,CAAC,MAAM,CAAC;YAE5C,mBAAmB;YACnB,IAAI,UAAU;gBACZ,MAAM,qJAAY,CAAC,GAAG,CAAC,UAAU,UAAU,UAAU,IAAI,CAAC,SAAS,EAAE,KAAK,CAAC,CAAC;oBAC1E,QAAQ,KAAK,CAAC,8BAA8B;gBAC9C;YACF;YAEA,SAAS,aAAa,GAAG,KAAK,GAAG,KAAK;YACtC,OAAO;QAET,EAAE,OAAO,OAAO;YACd,QAAQ,KAAK,CAAC,kBAAkB;YAChC,MAAM,IAAI,MAAM;QAClB;IACF;IAEA;;GAEC,GACD,MAAc,SAAS,QAAgB,EAAkC;QACvE,IAAI;YACF,MAAM,SAAS,MAAM,qJAAY,CAAC,GAAG,CAAiB,UAAU;YAChE,OAAO;QACT,EAAE,OAAO,OAAO;YACd,QAAQ,IAAI,CAAC,wBAAwB;YACrC,OAAO;QACT;IACF;IAEA;;GAEC,GACD,MAAM,UAAU,OAAiB,EAAiB;QAChD,QAAQ,GAAG,CAAC,CAAC,mBAAmB,EAAE,QAAQ,MAAM,CAAC,WAAW,CAAC;QAC7D,MAAM,WAAW,QAAQ,GAAG,CAAC,CAAC,QAC5B,IAAI,CAAC,MAAM,CAAC;gBAAE;gBAAO,MAAM;gBAAG,UAAU;gBAAI,UAAU;YAAK,GAAG,KAAK,CAAC,CAAC;gBACnE,QAAQ,KAAK,CAAC,CAAC,gCAAgC,EAAE,MAAM,EAAE,CAAC,EAAE;YAC9D;QAEF,MAAM,QAAQ,GAAG,CAAC;QAClB,QAAQ,GAAG,CAAC;IACd;IAEA;;GAEC,GACD,MAAM,gBAAgB,MAAoB,EAAiB;QACzD,MAAM,WAAW,IAAI,CAAC,WAAW,CAAC;QAClC,MAAM,qJAAY,CAAC,MAAM,CAAC,UAAU;IACtC;IAEA;;GAEC,GACD,MAAM,kBAIH;QACD,MAAM,WAAW,MAAM,IAAI,CAAC,mBAAmB,GAAG,KAAK,CAAC,IAAM;QAE9D,OAAO;YACL,eAAe;YACf,YAAY;YACZ,OAAO;QACT;IACF;IAEA;;GAEC,GACD,MAAc,sBAAwC;QACpD,IAAI;YACF,sBAAsB;YACtB,MAAM,uJAAa,CAAC,MAAM,CAAC;gBAAE,OAAO;gBAAQ,MAAM;gBAAG,UAAU;YAAE;YACjE,OAAO;QACT,EAAE,OAAM;YACN,OAAO;QACT;IACF;AACF;AAEO,MAAM,sBAAsB,IAAI"}},
    {"offset": {"line": 484, "column": 0}, "map": {"version":3,"sources":["file:///D:/search-engine-spec/lib/services/interaction-service.ts"],"sourcesContent":["import pool from '../db';\r\n\r\nexport interface SearchEvent {\r\n    user_id?: string;\r\n    session_id: string;\r\n    query: string;\r\n    result_count: number;\r\n    execution_time_ms: number;\r\n}\r\n\r\nexport interface ClickEvent {\r\n    search_event_id: string;\r\n    doc_id: string;\r\n    position: number;\r\n    user_id?: string;\r\n    session_id: string;\r\n}\r\n\r\nexport class InteractionService {\r\n    async logSearchEvent(event: SearchEvent): Promise<string> {\r\n        const sql = `\r\n            INSERT INTO search_events (\r\n                user_id, \r\n                session_id, \r\n                query_text, \r\n                result_count, \r\n                execution_time_ms\r\n            )\r\n            VALUES ($1, $2, $3, $4, $5)\r\n            RETURNING event_id\r\n        `;\r\n\r\n        try {\r\n            const result = await pool.query(sql, [\r\n                event.user_id || null,\r\n                event.session_id,\r\n                event.query,\r\n                event.result_count,\r\n                event.execution_time_ms\r\n            ]);\r\n            return result.rows[0].event_id;\r\n        } catch (error) {\r\n            console.error('Failed to log search event:', error);\r\n            return ''; // Fail gracefully\r\n        }\r\n    }\r\n\r\n    async logClickEvent(event: ClickEvent): Promise<string> {\r\n        const sql = `\r\n            INSERT INTO click_events (\r\n                search_event_id,\r\n                doc_id,\r\n                position\r\n            )\r\n            VALUES ($1, $2, $3)\r\n            RETURNING click_id\r\n        `;\r\n\r\n        try {\r\n            // Verify search_event_id exists to maintain referential integrity\r\n            // In a real high-volume system, we might skip this check or use a queue\r\n            if (event.search_event_id) {\r\n                const result = await pool.query(sql, [\r\n                    event.search_event_id,\r\n                    event.doc_id,\r\n                    event.position\r\n                ]);\r\n                return result.rows[0].click_id;\r\n            } else {\r\n                console.warn('Click event missing search_event_id, skipping insert');\r\n                return '';\r\n            }\r\n        } catch (error) {\r\n            console.error('Failed to log click event:', error);\r\n            return '';\r\n        }\r\n    }\r\n\r\n    async getUserHistory(userId: string, limit: number = 10) {\r\n        const sql = `\r\n            SELECT \r\n                se.query_text, \r\n                se.timestamp, \r\n                count(ce.click_id) as clicks\r\n            FROM search_events se\r\n            LEFT JOIN click_events ce ON se.event_id = ce.search_event_id\r\n            WHERE se.user_id = $1\r\n            GROUP BY se.event_id, se.query_text, se.timestamp\r\n            ORDER BY se.timestamp DESC\r\n            LIMIT $2\r\n        `;\r\n\r\n        try {\r\n            const result = await pool.query(sql, [userId, limit]);\r\n            return result.rows;\r\n        } catch (error) {\r\n            console.error('Failed to get user history:', error);\r\n            return [];\r\n        }\r\n    }\r\n}\r\n\r\nexport const interactionService = new InteractionService();\r\n"],"names":[],"mappings":";;;;;;AAAA;;AAkBO,MAAM;IACT,MAAM,eAAe,KAAkB,EAAmB;QACtD,MAAM,MAAM,CAAC;;;;;;;;;;QAUb,CAAC;QAED,IAAI;YACA,MAAM,SAAS,MAAM,sHAAI,CAAC,KAAK,CAAC,KAAK;gBACjC,MAAM,OAAO,IAAI;gBACjB,MAAM,UAAU;gBAChB,MAAM,KAAK;gBACX,MAAM,YAAY;gBAClB,MAAM,iBAAiB;aAC1B;YACD,OAAO,OAAO,IAAI,CAAC,EAAE,CAAC,QAAQ;QAClC,EAAE,OAAO,OAAO;YACZ,QAAQ,KAAK,CAAC,+BAA+B;YAC7C,OAAO,IAAI,kBAAkB;QACjC;IACJ;IAEA,MAAM,cAAc,KAAiB,EAAmB;QACpD,MAAM,MAAM,CAAC;;;;;;;;QAQb,CAAC;QAED,IAAI;YACA,kEAAkE;YAClE,wEAAwE;YACxE,IAAI,MAAM,eAAe,EAAE;gBACvB,MAAM,SAAS,MAAM,sHAAI,CAAC,KAAK,CAAC,KAAK;oBACjC,MAAM,eAAe;oBACrB,MAAM,MAAM;oBACZ,MAAM,QAAQ;iBACjB;gBACD,OAAO,OAAO,IAAI,CAAC,EAAE,CAAC,QAAQ;YAClC,OAAO;gBACH,QAAQ,IAAI,CAAC;gBACb,OAAO;YACX;QACJ,EAAE,OAAO,OAAO;YACZ,QAAQ,KAAK,CAAC,8BAA8B;YAC5C,OAAO;QACX;IACJ;IAEA,MAAM,eAAe,MAAc,EAAE,QAAgB,EAAE,EAAE;QACrD,MAAM,MAAM,CAAC;;;;;;;;;;;QAWb,CAAC;QAED,IAAI;YACA,MAAM,SAAS,MAAM,sHAAI,CAAC,KAAK,CAAC,KAAK;gBAAC;gBAAQ;aAAM;YACpD,OAAO,OAAO,IAAI;QACtB,EAAE,OAAO,OAAO;YACZ,QAAQ,KAAK,CAAC,+BAA+B;YAC7C,OAAO,EAAE;QACb;IACJ;AACJ;AAEO,MAAM,qBAAqB,IAAI"}},
    {"offset": {"line": 632, "column": 0}, "map": {"version":3,"sources":["file:///D:/search-engine-spec/lib/metrics/prometheus.ts"],"sourcesContent":["import { register, Counter, Histogram, Gauge } from 'prom-client';\r\n\r\n// Query metrics\r\nexport const queryCounter = new Counter({\r\n    name: 'search_queries_total',\r\n    help: 'Total number of search queries',\r\n    labelNames: ['status', 'cache_hit'],\r\n});\r\n\r\nexport const queryLatency = new Histogram({\r\n    name: 'search_query_duration_seconds',\r\n    help: 'Search query latency in seconds',\r\n    labelNames: ['stage'],\r\n    buckets: [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 5],\r\n});\r\n\r\nexport const queryResultCount = new Histogram({\r\n    name: 'search_query_results',\r\n    help: 'Number of results returned per query',\r\n    buckets: [0, 1, 10, 100, 1000, 10000],\r\n});\r\n\r\n// Indexing metrics\r\nexport const indexingCounter = new Counter({\r\n    name: 'documents_indexed_total',\r\n    help: 'Total number of documents indexed',\r\n    labelNames: ['status'],\r\n});\r\n\r\nexport const indexingLatency = new Histogram({\r\n    name: 'indexing_duration_seconds',\r\n    help: 'Document indexing latency in seconds',\r\n    buckets: [0.01, 0.05, 0.1, 0.5, 1, 2, 5],\r\n});\r\n\r\n// Crawler metrics\r\nexport const crawlCounter = new Counter({\r\n    name: 'urls_crawled_total',\r\n    help: 'Total number of URLs crawled',\r\n    labelNames: ['status'],\r\n});\r\n\r\nexport const crawlLatency = new Histogram({\r\n    name: 'crawl_duration_seconds',\r\n    help: 'URL crawl latency in seconds',\r\n    buckets: [0.1, 0.5, 1, 2, 5, 10, 30],\r\n});\r\n\r\nexport const crawlQueueSize = new Gauge({\r\n    name: 'crawl_queue_size',\r\n    help: 'Current size of the crawl queue',\r\n    labelNames: ['priority'],\r\n});\r\n\r\n// Cache metrics\r\nexport const cacheHitCounter = new Counter({\r\n    name: 'cache_hits_total',\r\n    help: 'Total number of cache hits',\r\n    labelNames: ['tier'],\r\n});\r\n\r\nexport const cacheMissCounter = new Counter({\r\n    name: 'cache_misses_total',\r\n    help: 'Total number of cache misses',\r\n    labelNames: ['tier'],\r\n});\r\n\r\n// System metrics\r\nexport const activeConnections = new Gauge({\r\n    name: 'active_connections',\r\n    help: 'Number of active connections',\r\n    labelNames: ['service'],\r\n});\r\n\r\nexport const errorCounter = new Counter({\r\n    name: 'errors_total',\r\n    help: 'Total number of errors',\r\n    labelNames: ['service', 'type'],\r\n});\r\n\r\n// Export metrics endpoint\r\nexport function getMetrics() {\r\n    return register.metrics();\r\n}\r\n\r\nexport { register };\r\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;;AAGO,MAAM,eAAe,IAAI,oJAAO,CAAC;IACpC,MAAM;IACN,MAAM;IACN,YAAY;QAAC;QAAU;KAAY;AACvC;AAEO,MAAM,eAAe,IAAI,sJAAS,CAAC;IACtC,MAAM;IACN,MAAM;IACN,YAAY;QAAC;KAAQ;IACrB,SAAS;QAAC;QAAM;QAAM;QAAK;QAAK;QAAK;QAAG;QAAG;KAAE;AACjD;AAEO,MAAM,mBAAmB,IAAI,sJAAS,CAAC;IAC1C,MAAM;IACN,MAAM;IACN,SAAS;QAAC;QAAG;QAAG;QAAI;QAAK;QAAM;KAAM;AACzC;AAGO,MAAM,kBAAkB,IAAI,oJAAO,CAAC;IACvC,MAAM;IACN,MAAM;IACN,YAAY;QAAC;KAAS;AAC1B;AAEO,MAAM,kBAAkB,IAAI,sJAAS,CAAC;IACzC,MAAM;IACN,MAAM;IACN,SAAS;QAAC;QAAM;QAAM;QAAK;QAAK;QAAG;QAAG;KAAE;AAC5C;AAGO,MAAM,eAAe,IAAI,oJAAO,CAAC;IACpC,MAAM;IACN,MAAM;IACN,YAAY;QAAC;KAAS;AAC1B;AAEO,MAAM,eAAe,IAAI,sJAAS,CAAC;IACtC,MAAM;IACN,MAAM;IACN,SAAS;QAAC;QAAK;QAAK;QAAG;QAAG;QAAG;QAAI;KAAG;AACxC;AAEO,MAAM,iBAAiB,IAAI,kJAAK,CAAC;IACpC,MAAM;IACN,MAAM;IACN,YAAY;QAAC;KAAW;AAC5B;AAGO,MAAM,kBAAkB,IAAI,oJAAO,CAAC;IACvC,MAAM;IACN,MAAM;IACN,YAAY;QAAC;KAAO;AACxB;AAEO,MAAM,mBAAmB,IAAI,oJAAO,CAAC;IACxC,MAAM;IACN,MAAM;IACN,YAAY;QAAC;KAAO;AACxB;AAGO,MAAM,oBAAoB,IAAI,kJAAK,CAAC;IACvC,MAAM;IACN,MAAM;IACN,YAAY;QAAC;KAAU;AAC3B;AAEO,MAAM,eAAe,IAAI,oJAAO,CAAC;IACpC,MAAM;IACN,MAAM;IACN,YAAY;QAAC;QAAW;KAAO;AACnC;AAGO,SAAS;IACZ,OAAO,qJAAQ,CAAC,OAAO;AAC3B"}},
    {"offset": {"line": 783, "column": 0}, "map": {"version":3,"sources":["file:///D:/search-engine-spec/lib/validation/schemas.ts"],"sourcesContent":["import { z } from 'zod';\r\n\r\n/**\r\n * Validation schemas for API request validation\r\n * Using Zod for type-safe runtime validation\r\n */\r\n\r\n// Search query validation\r\nexport const searchRequestSchema = z.object({\r\n  query: z\r\n    .string()\r\n    .trim()\r\n    .min(1, 'Query cannot be empty')\r\n    .max(500, 'Query too long (max 500 characters)')\r\n    .transform((val) => sanitizeQuery(val)),\r\n  page: z\r\n    .number()\r\n    .int()\r\n    .positive()\r\n    .max(100, 'Page number too high')\r\n    .default(1)\r\n    .optional(),\r\n  page_size: z\r\n    .number()\r\n    .int()\r\n    .positive()\r\n    .min(1)\r\n    .max(100, 'Page size too large')\r\n    .default(10)\r\n    .optional(),\r\n  filters: z\r\n    .object({\r\n      language: z.string().length(2).optional(),\r\n      site: z.string().url().or(z.string().regex(/^[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$/)).or(z.array(z.string())).optional(),\r\n      dateRange: z\r\n        .object({\r\n          start: z.string().datetime().optional(),\r\n          end: z.string().datetime().optional(),\r\n        })\r\n        .optional(),\r\n    })\r\n    .optional(),\r\n  options: z\r\n    .object({\r\n      spell_correct: z.boolean().optional(),\r\n      exact_match: z.boolean().optional(),\r\n      safe_search: z.boolean().default(true).optional(),\r\n    })\r\n    .optional(),\r\n});\r\n\r\nexport type SearchRequest = z.infer<typeof searchRequestSchema>;\r\n\r\n// Suggestion query validation\r\nexport const suggestionRequestSchema = z.object({\r\n  q: z\r\n    .string()\r\n    .trim()\r\n    .min(1, 'Query must be at least 1 character')\r\n    .max(200, 'Query too long')\r\n    .transform((val) => sanitizeQuery(val)),\r\n  limit: z.number().int().positive().max(20).default(10).optional(),\r\n});\r\n\r\nexport type SuggestionRequest = z.infer<typeof suggestionRequestSchema>;\r\n\r\n// Click event validation\r\nexport const clickEventSchema = z.object({\r\n  search_event_id: z.string().uuid('Invalid search event ID'),\r\n  doc_id: z.string().uuid('Invalid document ID'),\r\n  position: z.number().int().positive().max(1000),\r\n  url: z.string().url('Invalid URL'),\r\n  query: z.string().max(500).optional(),\r\n});\r\n\r\nexport type ClickEvent = z.infer<typeof clickEventSchema>;\r\n\r\n// Admin crawl request validation\r\nexport const crawlRequestSchema = z.object({\r\n  urls: z\r\n    .array(z.string().url('Invalid URL'))\r\n    .min(1, 'At least one URL required')\r\n    .max(1000, 'Too many URLs (max 1000)'),\r\n  priority: z.enum(['low', 'normal', 'high']).default('normal').optional(),\r\n  respect_robots: z.boolean().default(true).optional(),\r\n});\r\n\r\nexport type CrawlRequest = z.infer<typeof crawlRequestSchema>;\r\n\r\n/**\r\n * Sanitizes search query to prevent injection attacks\r\n * Removes potentially dangerous characters while preserving search functionality\r\n */\r\nfunction sanitizeQuery(query: string): string {\r\n  // Remove null bytes\r\n  let sanitized = query.replace(/\\0/g, '');\r\n\r\n  // Remove control characters except newline, carriage return, tab\r\n  sanitized = sanitized.replace(/[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]/g, '');\r\n\r\n  // Limit consecutive spaces\r\n  sanitized = sanitized.replace(/\\s+/g, ' ');\r\n\r\n  // Remove leading/trailing whitespace\r\n  sanitized = sanitized.trim();\r\n\r\n  // Remove potentially dangerous SQL patterns (defense in depth)\r\n  const dangerousPatterns = [\r\n    /;\\s*(drop|delete|insert|update|create|alter|exec|execute|script)/gi,\r\n    /--/g,\r\n    /\\/\\*/g,\r\n    /\\*\\//g,\r\n  ];\r\n\r\n  for (const pattern of dangerousPatterns) {\r\n    sanitized = sanitized.replace(pattern, '');\r\n  }\r\n\r\n  return sanitized;\r\n}\r\n\r\n/**\r\n * Sanitizes HTML content to prevent XSS attacks\r\n * Strips all HTML tags and converts special characters\r\n */\r\nexport function sanitizeHtml(html: string): string {\r\n  if (!html) return '';\r\n\r\n  // Strip HTML tags\r\n  let sanitized = html.replace(/<[^>]*>/g, '');\r\n\r\n  // Encode special characters\r\n  sanitized = sanitized\r\n    .replace(/&/g, '&amp;')\r\n    .replace(/</g, '&lt;')\r\n    .replace(/>/g, '&gt;')\r\n    .replace(/\"/g, '&quot;')\r\n    .replace(/'/g, '&#x27;')\r\n    .replace(/\\//g, '&#x2F;');\r\n\r\n  return sanitized;\r\n}\r\n\r\n/**\r\n * Validates and sanitizes URLs\r\n */\r\nexport function sanitizeUrl(url: string): string {\r\n  try {\r\n    const parsed = new URL(url);\r\n\r\n    // Only allow http and https protocols\r\n    if (!['http:', 'https:'].includes(parsed.protocol)) {\r\n      throw new Error('Invalid protocol');\r\n    }\r\n\r\n    return parsed.toString();\r\n  } catch {\r\n    throw new Error('Invalid URL format');\r\n  }\r\n}\r\n\r\n/**\r\n * Rate limit check result\r\n */\r\nexport interface RateLimitCheck {\r\n  allowed: boolean;\r\n  remaining: number;\r\n  resetAt: Date;\r\n}\r\n\r\n/**\r\n * Error response structure\r\n */\r\nexport interface ErrorResponse {\r\n  error: {\r\n    code: string;\r\n    message: string;\r\n    details?: any;\r\n  };\r\n  request_id: string;\r\n  timestamp: string;\r\n}\r\n\r\n/**\r\n * Creates a standardized error response\r\n */\r\nexport function createErrorResponse(\r\n  code: string,\r\n  message: string,\r\n  details?: any,\r\n  requestId?: string\r\n): ErrorResponse {\r\n  return {\r\n    error: {\r\n      code,\r\n      message,\r\n      details,\r\n    },\r\n    request_id: requestId || `req_${Date.now()}_${Math.random().toString(36).substring(7)}`,\r\n    timestamp: new Date().toISOString(),\r\n  };\r\n}\r\n"],"names":[],"mappings":";;;;;;;;;;;;;;;;AAAA;;AAQO,MAAM,sBAAsB,yKAAC,CAAC,MAAM,CAAC;IAC1C,OAAO,yKAAC,CACL,MAAM,GACN,IAAI,GACJ,GAAG,CAAC,GAAG,yBACP,GAAG,CAAC,KAAK,uCACT,SAAS,CAAC,CAAC,MAAQ,cAAc;IACpC,MAAM,yKAAC,CACJ,MAAM,GACN,GAAG,GACH,QAAQ,GACR,GAAG,CAAC,KAAK,wBACT,OAAO,CAAC,GACR,QAAQ;IACX,WAAW,yKAAC,CACT,MAAM,GACN,GAAG,GACH,QAAQ,GACR,GAAG,CAAC,GACJ,GAAG,CAAC,KAAK,uBACT,OAAO,CAAC,IACR,QAAQ;IACX,SAAS,yKAAC,CACP,MAAM,CAAC;QACN,UAAU,yKAAC,CAAC,MAAM,GAAG,MAAM,CAAC,GAAG,QAAQ;QACvC,MAAM,yKAAC,CAAC,MAAM,GAAG,GAAG,GAAG,EAAE,CAAC,yKAAC,CAAC,MAAM,GAAG,KAAK,CAAC,mCAAmC,EAAE,CAAC,yKAAC,CAAC,KAAK,CAAC,yKAAC,CAAC,MAAM,KAAK,QAAQ;QAC9G,WAAW,yKAAC,CACT,MAAM,CAAC;YACN,OAAO,yKAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,QAAQ;YACrC,KAAK,yKAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,QAAQ;QACrC,GACC,QAAQ;IACb,GACC,QAAQ;IACX,SAAS,yKAAC,CACP,MAAM,CAAC;QACN,eAAe,yKAAC,CAAC,OAAO,GAAG,QAAQ;QACnC,aAAa,yKAAC,CAAC,OAAO,GAAG,QAAQ;QACjC,aAAa,yKAAC,CAAC,OAAO,GAAG,OAAO,CAAC,MAAM,QAAQ;IACjD,GACC,QAAQ;AACb;AAKO,MAAM,0BAA0B,yKAAC,CAAC,MAAM,CAAC;IAC9C,GAAG,yKAAC,CACD,MAAM,GACN,IAAI,GACJ,GAAG,CAAC,GAAG,sCACP,GAAG,CAAC,KAAK,kBACT,SAAS,CAAC,CAAC,MAAQ,cAAc;IACpC,OAAO,yKAAC,CAAC,MAAM,GAAG,GAAG,GAAG,QAAQ,GAAG,GAAG,CAAC,IAAI,OAAO,CAAC,IAAI,QAAQ;AACjE;AAKO,MAAM,mBAAmB,yKAAC,CAAC,MAAM,CAAC;IACvC,iBAAiB,yKAAC,CAAC,MAAM,GAAG,IAAI,CAAC;IACjC,QAAQ,yKAAC,CAAC,MAAM,GAAG,IAAI,CAAC;IACxB,UAAU,yKAAC,CAAC,MAAM,GAAG,GAAG,GAAG,QAAQ,GAAG,GAAG,CAAC;IAC1C,KAAK,yKAAC,CAAC,MAAM,GAAG,GAAG,CAAC;IACpB,OAAO,yKAAC,CAAC,MAAM,GAAG,GAAG,CAAC,KAAK,QAAQ;AACrC;AAKO,MAAM,qBAAqB,yKAAC,CAAC,MAAM,CAAC;IACzC,MAAM,yKAAC,CACJ,KAAK,CAAC,yKAAC,CAAC,MAAM,GAAG,GAAG,CAAC,gBACrB,GAAG,CAAC,GAAG,6BACP,GAAG,CAAC,MAAM;IACb,UAAU,yKAAC,CAAC,IAAI,CAAC;QAAC;QAAO;QAAU;KAAO,EAAE,OAAO,CAAC,UAAU,QAAQ;IACtE,gBAAgB,yKAAC,CAAC,OAAO,GAAG,OAAO,CAAC,MAAM,QAAQ;AACpD;AAIA;;;CAGC,GACD,SAAS,cAAc,KAAa;IAClC,oBAAoB;IACpB,IAAI,YAAY,MAAM,OAAO,CAAC,OAAO;IAErC,iEAAiE;IACjE,YAAY,UAAU,OAAO,CAAC,qCAAqC;IAEnE,2BAA2B;IAC3B,YAAY,UAAU,OAAO,CAAC,QAAQ;IAEtC,qCAAqC;IACrC,YAAY,UAAU,IAAI;IAE1B,+DAA+D;IAC/D,MAAM,oBAAoB;QACxB;QACA;QACA;QACA;KACD;IAED,KAAK,MAAM,WAAW,kBAAmB;QACvC,YAAY,UAAU,OAAO,CAAC,SAAS;IACzC;IAEA,OAAO;AACT;AAMO,SAAS,aAAa,IAAY;IACvC,IAAI,CAAC,MAAM,OAAO;IAElB,kBAAkB;IAClB,IAAI,YAAY,KAAK,OAAO,CAAC,YAAY;IAEzC,4BAA4B;IAC5B,YAAY,UACT,OAAO,CAAC,MAAM,SACd,OAAO,CAAC,MAAM,QACd,OAAO,CAAC,MAAM,QACd,OAAO,CAAC,MAAM,UACd,OAAO,CAAC,MAAM,UACd,OAAO,CAAC,OAAO;IAElB,OAAO;AACT;AAKO,SAAS,YAAY,GAAW;IACrC,IAAI;QACF,MAAM,SAAS,IAAI,IAAI;QAEvB,sCAAsC;QACtC,IAAI,CAAC;YAAC;YAAS;SAAS,CAAC,QAAQ,CAAC,OAAO,QAAQ,GAAG;YAClD,MAAM,IAAI,MAAM;QAClB;QAEA,OAAO,OAAO,QAAQ;IACxB,EAAE,OAAM;QACN,MAAM,IAAI,MAAM;IAClB;AACF;AA2BO,SAAS,oBACd,IAAY,EACZ,OAAe,EACf,OAAa,EACb,SAAkB;IAElB,OAAO;QACL,OAAO;YACL;YACA;YACA;QACF;QACA,YAAY,aAAa,CAAC,IAAI,EAAE,KAAK,GAAG,GAAG,CAAC,EAAE,KAAK,MAAM,GAAG,QAAQ,CAAC,IAAI,SAAS,CAAC,IAAI;QACvF,WAAW,IAAI,OAAO,WAAW;IACnC;AACF"}},
    {"offset": {"line": 901, "column": 0}, "map": {"version":3,"sources":["file:///D:/search-engine-spec/lib/middleware/rate-limit.ts"],"sourcesContent":["import { NextRequest, NextResponse } from 'next/server';\r\nimport { createErrorResponse, RateLimitCheck } from '../validation/schemas';\r\n\r\n/**\r\n * Rate Limiting Middleware\r\n * Implements sliding window rate limiting with in-memory store\r\n * For production, integrate with Redis for distributed rate limiting\r\n */\r\n\r\ninterface RateLimitConfig {\r\n  windowMs: number; // Time window in milliseconds\r\n  maxRequests: number; // Maximum requests per window\r\n  keyGenerator?: (req: NextRequest) => string; // Custom key generator\r\n  skipSuccessfulRequests?: boolean; // Don't count successful requests\r\n  skipFailedRequests?: boolean; // Don't count failed requests\r\n}\r\n\r\ninterface RequestRecord {\r\n  timestamps: number[];\r\n  blockedUntil?: number;\r\n}\r\n\r\nclass RateLimiter {\r\n  private store: Map<string, RequestRecord> = new Map();\r\n  private cleanupInterval: NodeJS.Timeout;\r\n\r\n  constructor() {\r\n    // Cleanup old entries every 5 minutes\r\n    this.cleanupInterval = setInterval(() => {\r\n      this.cleanup();\r\n    }, 5 * 60 * 1000);\r\n  }\r\n\r\n  /**\r\n   * Check if request is allowed under rate limit\r\n   */\r\n  check(key: string, config: RateLimitConfig): RateLimitCheck {\r\n    const now = Date.now();\r\n    const windowStart = now - config.windowMs;\r\n\r\n    // Get or create record\r\n    let record = this.store.get(key);\r\n    if (!record) {\r\n      record = { timestamps: [] };\r\n      this.store.set(key, record);\r\n    }\r\n\r\n    // Check if currently blocked\r\n    if (record.blockedUntil && record.blockedUntil > now) {\r\n      return {\r\n        allowed: false,\r\n        remaining: 0,\r\n        resetAt: new Date(record.blockedUntil),\r\n      };\r\n    }\r\n\r\n    // Remove timestamps outside the window\r\n    record.timestamps = record.timestamps.filter((ts) => ts > windowStart);\r\n\r\n    // Check if limit exceeded\r\n    if (record.timestamps.length >= config.maxRequests) {\r\n      // Block for remaining window time\r\n      const oldestTimestamp = Math.min(...record.timestamps);\r\n      const resetAt = oldestTimestamp + config.windowMs;\r\n      record.blockedUntil = resetAt;\r\n\r\n      return {\r\n        allowed: false,\r\n        remaining: 0,\r\n        resetAt: new Date(resetAt),\r\n      };\r\n    }\r\n\r\n    // Add current request\r\n    record.timestamps.push(now);\r\n\r\n    return {\r\n      allowed: true,\r\n      remaining: config.maxRequests - record.timestamps.length,\r\n      resetAt: new Date(now + config.windowMs),\r\n    };\r\n  }\r\n\r\n  /**\r\n   * Cleanup old entries from store\r\n   */\r\n  private cleanup() {\r\n    const now = Date.now();\r\n    const maxAge = 3600000; // 1 hour\r\n\r\n    for (const [key, record] of this.store.entries()) {\r\n      // Remove if no recent activity and not blocked\r\n      if (\r\n        (!record.blockedUntil || record.blockedUntil < now) &&\r\n        record.timestamps.length === 0\r\n      ) {\r\n        this.store.delete(key);\r\n        continue;\r\n      }\r\n\r\n      // Remove if last activity was over max age\r\n      if (record.timestamps.length > 0) {\r\n        const lastTimestamp = Math.max(...record.timestamps);\r\n        if (now - lastTimestamp > maxAge) {\r\n          this.store.delete(key);\r\n        }\r\n      }\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Reset rate limit for a key\r\n   */\r\n  reset(key: string) {\r\n    this.store.delete(key);\r\n  }\r\n\r\n  /**\r\n   * Shutdown cleanup interval\r\n   */\r\n  destroy() {\r\n    clearInterval(this.cleanupInterval);\r\n  }\r\n}\r\n\r\n// Global rate limiter instance\r\nconst rateLimiter = new RateLimiter();\r\n\r\n/**\r\n * Default rate limit configurations for different endpoints\r\n */\r\nexport const rateLimitConfigs = {\r\n  // Search API: 60 requests per minute per IP\r\n  search: {\r\n    windowMs: 60 * 1000,\r\n    maxRequests: 60,\r\n  },\r\n  // Suggestions API: 120 requests per minute per IP (more lenient for autocomplete)\r\n  suggest: {\r\n    windowMs: 60 * 1000,\r\n    maxRequests: 120,\r\n  },\r\n  // Admin APIs: 10 requests per minute per IP\r\n  admin: {\r\n    windowMs: 60 * 1000,\r\n    maxRequests: 10,\r\n  },\r\n  // Click tracking: 100 requests per minute per IP\r\n  click: {\r\n    windowMs: 60 * 1000,\r\n    maxRequests: 100,\r\n  },\r\n};\r\n\r\n/**\r\n * Get client identifier from request\r\n * Uses IP address, falling back to user-agent if IP unavailable\r\n */\r\nfunction getClientKey(req: NextRequest, prefix: string = 'rl'): string {\r\n  // Get IP address\r\n  const forwarded = req.headers.get('x-forwarded-for');\r\n  const realIp = req.headers.get('x-real-ip');\r\n  const ip = forwarded?.split(',')[0] || realIp || req.ip || 'unknown';\r\n\r\n  // Include user agent for better uniqueness\r\n  const userAgent = req.headers.get('user-agent') || 'unknown';\r\n  const userAgentHash = simpleHash(userAgent);\r\n\r\n  return `${prefix}:${ip}:${userAgentHash}`;\r\n}\r\n\r\n/**\r\n * Simple hash function for strings\r\n */\r\nfunction simpleHash(str: string): string {\r\n  let hash = 0;\r\n  for (let i = 0; i < str.length; i++) {\r\n    const char = str.charCodeAt(i);\r\n    hash = (hash << 5) - hash + char;\r\n    hash = hash & hash; // Convert to 32-bit integer\r\n  }\r\n  return Math.abs(hash).toString(36);\r\n}\r\n\r\n/**\r\n * Rate limit middleware factory\r\n * Creates middleware function with specified configuration\r\n */\r\nexport function createRateLimitMiddleware(config: RateLimitConfig) {\r\n  return async (req: NextRequest): Promise<NextResponse | null> => {\r\n    // Generate rate limit key\r\n    const key = config.keyGenerator?.(req) || getClientKey(req);\r\n\r\n    // Check rate limit\r\n    const result = rateLimiter.check(key, config);\r\n\r\n    // Add rate limit headers\r\n    const headers = new Headers();\r\n    headers.set('X-RateLimit-Limit', config.maxRequests.toString());\r\n    headers.set('X-RateLimit-Remaining', result.remaining.toString());\r\n    headers.set('X-RateLimit-Reset', result.resetAt.toISOString());\r\n\r\n    // If rate limit exceeded, return 429\r\n    if (!result.allowed) {\r\n      const retryAfter = Math.ceil((result.resetAt.getTime() - Date.now()) / 1000);\r\n      headers.set('Retry-After', retryAfter.toString());\r\n\r\n      return NextResponse.json(\r\n        createErrorResponse(\r\n          'RATE_LIMIT_EXCEEDED',\r\n          'Too many requests. Please try again later.',\r\n          {\r\n            limit: config.maxRequests,\r\n            window_ms: config.windowMs,\r\n            retry_after_seconds: retryAfter,\r\n          }\r\n        ),\r\n        {\r\n          status: 429,\r\n          headers,\r\n        }\r\n      );\r\n    }\r\n\r\n    return null; // Allow request to proceed\r\n  };\r\n}\r\n\r\n/**\r\n * Apply rate limiting to a handler function\r\n */\r\nexport function withRateLimit(\r\n  handler: (req: NextRequest) => Promise<NextResponse>,\r\n  config: RateLimitConfig\r\n) {\r\n  return async (req: NextRequest): Promise<NextResponse> => {\r\n    const rateLimitMiddleware = createRateLimitMiddleware(config);\r\n    const rateLimitResponse = await rateLimitMiddleware(req);\r\n\r\n    if (rateLimitResponse) {\r\n      return rateLimitResponse;\r\n    }\r\n\r\n    return handler(req);\r\n  };\r\n}\r\n\r\n/**\r\n * Session-based rate limiting (in addition to IP-based)\r\n * More lenient limits for authenticated users\r\n */\r\nexport function getSessionKey(req: NextRequest): string {\r\n  const sessionId = req.headers.get('x-session-id');\r\n  const userId = req.headers.get('x-user-id');\r\n\r\n  if (userId) {\r\n    return `rl:user:${userId}`;\r\n  } else if (sessionId) {\r\n    return `rl:session:${sessionId}`;\r\n  } else {\r\n    return getClientKey(req);\r\n  }\r\n}\r\n\r\n/**\r\n * Export rate limiter for testing\r\n */\r\nexport { rateLimiter };\r\n"],"names":[],"mappings":";;;;;;;;;;;;AAAA;AACA;;;AAqBA,MAAM;IACI,QAAoC,IAAI,MAAM;IAC9C,gBAAgC;IAExC,aAAc;QACZ,sCAAsC;QACtC,IAAI,CAAC,eAAe,GAAG,YAAY;YACjC,IAAI,CAAC,OAAO;QACd,GAAG,IAAI,KAAK;IACd;IAEA;;GAEC,GACD,MAAM,GAAW,EAAE,MAAuB,EAAkB;QAC1D,MAAM,MAAM,KAAK,GAAG;QACpB,MAAM,cAAc,MAAM,OAAO,QAAQ;QAEzC,uBAAuB;QACvB,IAAI,SAAS,IAAI,CAAC,KAAK,CAAC,GAAG,CAAC;QAC5B,IAAI,CAAC,QAAQ;YACX,SAAS;gBAAE,YAAY,EAAE;YAAC;YAC1B,IAAI,CAAC,KAAK,CAAC,GAAG,CAAC,KAAK;QACtB;QAEA,6BAA6B;QAC7B,IAAI,OAAO,YAAY,IAAI,OAAO,YAAY,GAAG,KAAK;YACpD,OAAO;gBACL,SAAS;gBACT,WAAW;gBACX,SAAS,IAAI,KAAK,OAAO,YAAY;YACvC;QACF;QAEA,uCAAuC;QACvC,OAAO,UAAU,GAAG,OAAO,UAAU,CAAC,MAAM,CAAC,CAAC,KAAO,KAAK;QAE1D,0BAA0B;QAC1B,IAAI,OAAO,UAAU,CAAC,MAAM,IAAI,OAAO,WAAW,EAAE;YAClD,kCAAkC;YAClC,MAAM,kBAAkB,KAAK,GAAG,IAAI,OAAO,UAAU;YACrD,MAAM,UAAU,kBAAkB,OAAO,QAAQ;YACjD,OAAO,YAAY,GAAG;YAEtB,OAAO;gBACL,SAAS;gBACT,WAAW;gBACX,SAAS,IAAI,KAAK;YACpB;QACF;QAEA,sBAAsB;QACtB,OAAO,UAAU,CAAC,IAAI,CAAC;QAEvB,OAAO;YACL,SAAS;YACT,WAAW,OAAO,WAAW,GAAG,OAAO,UAAU,CAAC,MAAM;YACxD,SAAS,IAAI,KAAK,MAAM,OAAO,QAAQ;QACzC;IACF;IAEA;;GAEC,GACD,AAAQ,UAAU;QAChB,MAAM,MAAM,KAAK,GAAG;QACpB,MAAM,SAAS,SAAS,SAAS;QAEjC,KAAK,MAAM,CAAC,KAAK,OAAO,IAAI,IAAI,CAAC,KAAK,CAAC,OAAO,GAAI;YAChD,+CAA+C;YAC/C,IACE,CAAC,CAAC,OAAO,YAAY,IAAI,OAAO,YAAY,GAAG,GAAG,KAClD,OAAO,UAAU,CAAC,MAAM,KAAK,GAC7B;gBACA,IAAI,CAAC,KAAK,CAAC,MAAM,CAAC;gBAClB;YACF;YAEA,2CAA2C;YAC3C,IAAI,OAAO,UAAU,CAAC,MAAM,GAAG,GAAG;gBAChC,MAAM,gBAAgB,KAAK,GAAG,IAAI,OAAO,UAAU;gBACnD,IAAI,MAAM,gBAAgB,QAAQ;oBAChC,IAAI,CAAC,KAAK,CAAC,MAAM,CAAC;gBACpB;YACF;QACF;IACF;IAEA;;GAEC,GACD,MAAM,GAAW,EAAE;QACjB,IAAI,CAAC,KAAK,CAAC,MAAM,CAAC;IACpB;IAEA;;GAEC,GACD,UAAU;QACR,cAAc,IAAI,CAAC,eAAe;IACpC;AACF;AAEA,+BAA+B;AAC/B,MAAM,cAAc,IAAI;AAKjB,MAAM,mBAAmB;IAC9B,4CAA4C;IAC5C,QAAQ;QACN,UAAU,KAAK;QACf,aAAa;IACf;IACA,kFAAkF;IAClF,SAAS;QACP,UAAU,KAAK;QACf,aAAa;IACf;IACA,4CAA4C;IAC5C,OAAO;QACL,UAAU,KAAK;QACf,aAAa;IACf;IACA,iDAAiD;IACjD,OAAO;QACL,UAAU,KAAK;QACf,aAAa;IACf;AACF;AAEA;;;CAGC,GACD,SAAS,aAAa,GAAgB,EAAE,SAAiB,IAAI;IAC3D,iBAAiB;IACjB,MAAM,YAAY,IAAI,OAAO,CAAC,GAAG,CAAC;IAClC,MAAM,SAAS,IAAI,OAAO,CAAC,GAAG,CAAC;IAC/B,MAAM,KAAK,WAAW,MAAM,IAAI,CAAC,EAAE,IAAI,UAAU,IAAI,EAAE,IAAI;IAE3D,2CAA2C;IAC3C,MAAM,YAAY,IAAI,OAAO,CAAC,GAAG,CAAC,iBAAiB;IACnD,MAAM,gBAAgB,WAAW;IAEjC,OAAO,GAAG,OAAO,CAAC,EAAE,GAAG,CAAC,EAAE,eAAe;AAC3C;AAEA;;CAEC,GACD,SAAS,WAAW,GAAW;IAC7B,IAAI,OAAO;IACX,IAAK,IAAI,IAAI,GAAG,IAAI,IAAI,MAAM,EAAE,IAAK;QACnC,MAAM,OAAO,IAAI,UAAU,CAAC;QAC5B,OAAO,CAAC,QAAQ,CAAC,IAAI,OAAO;QAC5B,OAAO,OAAO,MAAM,4BAA4B;IAClD;IACA,OAAO,KAAK,GAAG,CAAC,MAAM,QAAQ,CAAC;AACjC;AAMO,SAAS,0BAA0B,MAAuB;IAC/D,OAAO,OAAO;QACZ,0BAA0B;QAC1B,MAAM,MAAM,OAAO,YAAY,GAAG,QAAQ,aAAa;QAEvD,mBAAmB;QACnB,MAAM,SAAS,YAAY,KAAK,CAAC,KAAK;QAEtC,yBAAyB;QACzB,MAAM,UAAU,IAAI;QACpB,QAAQ,GAAG,CAAC,qBAAqB,OAAO,WAAW,CAAC,QAAQ;QAC5D,QAAQ,GAAG,CAAC,yBAAyB,OAAO,SAAS,CAAC,QAAQ;QAC9D,QAAQ,GAAG,CAAC,qBAAqB,OAAO,OAAO,CAAC,WAAW;QAE3D,qCAAqC;QACrC,IAAI,CAAC,OAAO,OAAO,EAAE;YACnB,MAAM,aAAa,KAAK,IAAI,CAAC,CAAC,OAAO,OAAO,CAAC,OAAO,KAAK,KAAK,GAAG,EAAE,IAAI;YACvE,QAAQ,GAAG,CAAC,eAAe,WAAW,QAAQ;YAE9C,OAAO,gJAAY,CAAC,IAAI,CACtB,IAAA,qJAAmB,EACjB,uBACA,8CACA;gBACE,OAAO,OAAO,WAAW;gBACzB,WAAW,OAAO,QAAQ;gBAC1B,qBAAqB;YACvB,IAEF;gBACE,QAAQ;gBACR;YACF;QAEJ;QAEA,OAAO,MAAM,2BAA2B;IAC1C;AACF;AAKO,SAAS,cACd,OAAoD,EACpD,MAAuB;IAEvB,OAAO,OAAO;QACZ,MAAM,sBAAsB,0BAA0B;QACtD,MAAM,oBAAoB,MAAM,oBAAoB;QAEpD,IAAI,mBAAmB;YACrB,OAAO;QACT;QAEA,OAAO,QAAQ;IACjB;AACF;AAMO,SAAS,cAAc,GAAgB;IAC5C,MAAM,YAAY,IAAI,OAAO,CAAC,GAAG,CAAC;IAClC,MAAM,SAAS,IAAI,OAAO,CAAC,GAAG,CAAC;IAE/B,IAAI,QAAQ;QACV,OAAO,CAAC,QAAQ,EAAE,QAAQ;IAC5B,OAAO,IAAI,WAAW;QACpB,OAAO,CAAC,WAAW,EAAE,WAAW;IAClC,OAAO;QACL,OAAO,aAAa;IACtB;AACF"}},
    {"offset": {"line": 1101, "column": 0}, "map": {"version":3,"sources":["file:///D:/search-engine-spec/app/api/v1/search/route.ts"],"sourcesContent":["import { NextRequest, NextResponse } from 'next/server';\r\nimport { hybridSearchService } from '@/lib/services/hybrid-search-service';\r\nimport { interactionService } from '@/lib/services/interaction-service';\r\nimport { queryCounter, queryLatency, queryResultCount } from '@/lib/metrics/prometheus';\r\nimport { searchRequestSchema, createErrorResponse } from '@/lib/validation/schemas';\r\nimport { withRateLimit, rateLimitConfigs } from '@/lib/middleware/rate-limit';\r\n\r\n/**\r\n * Search API endpoint\r\n * POST /api/v1/search\r\n * \r\n * Features:\r\n * - Input validation and sanitization\r\n * - Rate limiting (60 req/min per IP)\r\n * - Hybrid search (Cache â†’ Elasticsearch â†’ PostgreSQL)\r\n * - Comprehensive error handling\r\n * - Metrics collection\r\n */\r\n\r\nasync function searchHandler(req: NextRequest) {\r\n    const startTime = Date.now();\r\n    const requestId = `req_${Date.now()}_${Math.random().toString(36).substring(7)}`;\r\n\r\n    try {\r\n        // Parse and validate request body\r\n        const body = await req.json();\r\n        const validation = searchRequestSchema.safeParse(body);\r\n\r\n        if (!validation.success) {\r\n            queryCounter.inc({ status: 'error', cache_hit: 'false' });\r\n            return NextResponse.json(\r\n                createErrorResponse(\r\n                    'VALIDATION_ERROR',\r\n                    'Invalid request parameters',\r\n                    validation.error.errors,\r\n                    requestId\r\n                ),\r\n                { status: 400 }\r\n            );\r\n        }\r\n\r\n        const { query, page, page_size, filters, options } = validation.data;\r\n\r\n        // Execute hybrid search (Cache â†’ PostgreSQL fallback, skip Elasticsearch)\r\n        const searchStart = Date.now();\r\n        const results = await hybridSearchService.search({\r\n            query,\r\n            page,\r\n            pageSize: page_size,\r\n            filters,\r\n            useCache: true,\r\n            preferElasticsearch: process.env.ENABLE_ELASTICSEARCH === 'true', // Configurable via env\r\n        });\r\n        const executionTime = Date.now() - searchStart;\r\n\r\n        // Log search event\r\n        const sessionId = req.headers.get('x-session-id') || 'anonymous';\r\n        const userId = req.headers.get('x-user-id') || undefined;\r\n\r\n        let eventId = '';\r\n        try {\r\n            eventId = await interactionService.logSearchEvent({\r\n                user_id: userId,\r\n                session_id: sessionId,\r\n                query,\r\n                result_count: results.total_results,\r\n                execution_time_ms: executionTime,\r\n            });\r\n        } catch (err) {\r\n            console.error('Error logging search event:', err);\r\n        }\r\n\r\n        // Record metrics\r\n        queryCounter.inc({ status: 'success', cache_hit: 'false' });\r\n        queryResultCount.observe(results.results.length);\r\n        queryLatency.observe({ stage: 'total' }, (Date.now() - startTime) / 1000);\r\n\r\n        // Add response headers\r\n        const headers = new Headers();\r\n        headers.set('X-Request-ID', requestId);\r\n        headers.set('X-Query-Time-Ms', executionTime.toString());\r\n\r\n        return NextResponse.json(\r\n            {\r\n                ...results,\r\n                meta: {\r\n                    search_event_id: eventId,\r\n                    request_id: requestId,\r\n                },\r\n            },\r\n            { headers }\r\n        );\r\n    } catch (error: any) {\r\n        console.error('Search API error:', error);\r\n        queryCounter.inc({ status: 'error', cache_hit: 'false' });\r\n\r\n        // Determine error type and status code\r\n        let statusCode = 500;\r\n        let errorCode = 'INTERNAL_ERROR';\r\n        let errorMessage = 'An internal server error occurred';\r\n\r\n        if (error.message === 'Search service temporarily unavailable') {\r\n            statusCode = 503;\r\n            errorCode = 'SERVICE_UNAVAILABLE';\r\n            errorMessage = error.message;\r\n        }\r\n\r\n        return NextResponse.json(\r\n            createErrorResponse(\r\n                errorCode,\r\n                errorMessage,\r\n                process.env.NODE_ENV === 'development' ? error.message : undefined,\r\n                requestId\r\n            ),\r\n            { status: statusCode }\r\n        );\r\n    }\r\n}\r\n\r\n// Export with rate limiting\r\nexport const POST = withRateLimit(searchHandler, rateLimitConfigs.search);\r\n"],"names":[],"mappings":";;;;AAAA;AACA;AACA;AACA;AACA;AACA;;;;;;;AAEA;;;;;;;;;;CAUC,GAED,eAAe,cAAc,GAAgB;IACzC,MAAM,YAAY,KAAK,GAAG;IAC1B,MAAM,YAAY,CAAC,IAAI,EAAE,KAAK,GAAG,GAAG,CAAC,EAAE,KAAK,MAAM,GAAG,QAAQ,CAAC,IAAI,SAAS,CAAC,IAAI;IAEhF,IAAI;QACA,kCAAkC;QAClC,MAAM,OAAO,MAAM,IAAI,IAAI;QAC3B,MAAM,aAAa,qJAAmB,CAAC,SAAS,CAAC;QAEjD,IAAI,CAAC,WAAW,OAAO,EAAE;YACrB,8JAAY,CAAC,GAAG,CAAC;gBAAE,QAAQ;gBAAS,WAAW;YAAQ;YACvD,OAAO,gJAAY,CAAC,IAAI,CACpB,IAAA,qJAAmB,EACf,oBACA,8BACA,WAAW,KAAK,CAAC,MAAM,EACvB,YAEJ;gBAAE,QAAQ;YAAI;QAEtB;QAEA,MAAM,EAAE,KAAK,EAAE,IAAI,EAAE,SAAS,EAAE,OAAO,EAAE,OAAO,EAAE,GAAG,WAAW,IAAI;QAEpE,0EAA0E;QAC1E,MAAM,cAAc,KAAK,GAAG;QAC5B,MAAM,UAAU,MAAM,uKAAmB,CAAC,MAAM,CAAC;YAC7C;YACA;YACA,UAAU;YACV;YACA,UAAU;YACV,qBAAqB,QAAQ,GAAG,CAAC,oBAAoB,KAAK;QAC9D;QACA,MAAM,gBAAgB,KAAK,GAAG,KAAK;QAEnC,mBAAmB;QACnB,MAAM,YAAY,IAAI,OAAO,CAAC,GAAG,CAAC,mBAAmB;QACrD,MAAM,SAAS,IAAI,OAAO,CAAC,GAAG,CAAC,gBAAgB;QAE/C,IAAI,UAAU;QACd,IAAI;YACA,UAAU,MAAM,iKAAkB,CAAC,cAAc,CAAC;gBAC9C,SAAS;gBACT,YAAY;gBACZ;gBACA,cAAc,QAAQ,aAAa;gBACnC,mBAAmB;YACvB;QACJ,EAAE,OAAO,KAAK;YACV,QAAQ,KAAK,CAAC,+BAA+B;QACjD;QAEA,iBAAiB;QACjB,8JAAY,CAAC,GAAG,CAAC;YAAE,QAAQ;YAAW,WAAW;QAAQ;QACzD,kKAAgB,CAAC,OAAO,CAAC,QAAQ,OAAO,CAAC,MAAM;QAC/C,8JAAY,CAAC,OAAO,CAAC;YAAE,OAAO;QAAQ,GAAG,CAAC,KAAK,GAAG,KAAK,SAAS,IAAI;QAEpE,uBAAuB;QACvB,MAAM,UAAU,IAAI;QACpB,QAAQ,GAAG,CAAC,gBAAgB;QAC5B,QAAQ,GAAG,CAAC,mBAAmB,cAAc,QAAQ;QAErD,OAAO,gJAAY,CAAC,IAAI,CACpB;YACI,GAAG,OAAO;YACV,MAAM;gBACF,iBAAiB;gBACjB,YAAY;YAChB;QACJ,GACA;YAAE;QAAQ;IAElB,EAAE,OAAO,OAAY;QACjB,QAAQ,KAAK,CAAC,qBAAqB;QACnC,8JAAY,CAAC,GAAG,CAAC;YAAE,QAAQ;YAAS,WAAW;QAAQ;QAEvD,uCAAuC;QACvC,IAAI,aAAa;QACjB,IAAI,YAAY;QAChB,IAAI,eAAe;QAEnB,IAAI,MAAM,OAAO,KAAK,0CAA0C;YAC5D,aAAa;YACb,YAAY;YACZ,eAAe,MAAM,OAAO;QAChC;QAEA,OAAO,gJAAY,CAAC,IAAI,CACpB,IAAA,qJAAmB,EACf,WACA,cACA,uCAAyC,MAAM,OAAO,GAAG,yBACzD,YAEJ;YAAE,QAAQ;QAAW;IAE7B;AACJ;AAGO,MAAM,OAAO,IAAA,qJAAa,EAAC,eAAe,wJAAgB,CAAC,MAAM"}}]
}