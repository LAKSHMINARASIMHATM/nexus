{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 64, "column": 0}, "map": {"version":3,"sources":["file:///D:/search-engine-spec/lib/db.ts"],"sourcesContent":["import { Pool } from '@neondatabase/serverless';\r\nimport * as dotenv from 'dotenv';\r\n\r\ndotenv.config({ path: '.env.local' });\r\ndotenv.config({ path: '.env' });\r\n\r\nconsole.log('Initializing Neon DB pool...');\r\n\r\nconst pool = new Pool({\r\n    connectionString: process.env.DATABASE_URL || 'postgresql://postgres:postgres@localhost:5432/search_engine',\r\n    ssl: true,\r\n});\r\n\r\nexport default pool;\r\n"],"names":[],"mappings":";;;;AAAA;AACA;;;AAEA,iJAAa,CAAC;IAAE,MAAM;AAAa;AACnC,iJAAa,CAAC;IAAE,MAAM;AAAO;AAE7B,QAAQ,GAAG,CAAC;AAEZ,MAAM,OAAO,IAAI,gKAAI,CAAC;IAClB,kBAAkB,QAAQ,GAAG,CAAC,YAAY,IAAI;IAC9C,KAAK;AACT;uCAEe"}},
    {"offset": {"line": 88, "column": 0}, "map": {"version":3,"sources":["file:///D:/search-engine-spec/lib/services/crawler-service.ts"],"sourcesContent":["import pool from '../db';\r\n\r\nexport class CrawlerService {\r\n    async queueUrls(urls: string[], priority: number = 5): Promise<string> {\r\n        const jobId = `crawl-job-${Date.now()}`;\r\n\r\n        // In a real system, we would push to Kafka/Queue. Here we insert into DB.\r\n        const client = await pool.connect();\r\n        try {\r\n            await client.query('BEGIN');\r\n            for (const url of urls) {\r\n                await client.query(\r\n                    `INSERT INTO crawl_queue (url, priority, status, metadata) \r\n           VALUES ($1, $2, 'pending', $3)\r\n           ON CONFLICT (url) DO UPDATE SET priority = $2, status = 'pending'`,\r\n                    [url, priority, JSON.stringify({ jobId })]\r\n                );\r\n            }\r\n            await client.query('COMMIT');\r\n            return jobId;\r\n        } catch (error) {\r\n            await client.query('ROLLBACK');\r\n            throw error;\r\n        } finally {\r\n            client.release();\r\n        }\r\n    }\r\n\r\n    async getJobStatus(jobId: string) {\r\n        // This is a simplified status check assuming we track job_id in metadata\r\n        // In reality, we'd have a separate jobs table or more complex tracking\r\n        const sql = `\r\n      SELECT \r\n        COUNT(*) as total,\r\n        COUNT(*) FILTER (WHERE status = 'completed') as completed,\r\n        COUNT(*) FILTER (WHERE status = 'failed') as failed,\r\n        COUNT(*) FILTER (WHERE status = 'in_progress') as in_progress\r\n      FROM crawl_queue\r\n      WHERE metadata->>'jobId' = $1\r\n    `;\r\n\r\n        const result = await pool.query(sql, [jobId]);\r\n        const row = result.rows[0];\r\n\r\n        return {\r\n            job_id: jobId,\r\n            status: row.completed == row.total && row.total > 0 ? 'completed' : 'in_progress',\r\n            progress: {\r\n                total_urls: parseInt(row.total),\r\n                completed: parseInt(row.completed),\r\n                failed: parseInt(row.failed),\r\n                in_progress: parseInt(row.in_progress)\r\n            }\r\n        };\r\n    }\r\n}\r\n\r\nexport const crawlerService = new CrawlerService();\r\n"],"names":[],"mappings":";;;;;;AAAA;;AAEO,MAAM;IACT,MAAM,UAAU,IAAc,EAAE,WAAmB,CAAC,EAAmB;QACnE,MAAM,QAAQ,CAAC,UAAU,EAAE,KAAK,GAAG,IAAI;QAEvC,0EAA0E;QAC1E,MAAM,SAAS,MAAM,sHAAI,CAAC,OAAO;QACjC,IAAI;YACA,MAAM,OAAO,KAAK,CAAC;YACnB,KAAK,MAAM,OAAO,KAAM;gBACpB,MAAM,OAAO,KAAK,CACd,CAAC;;4EAEuD,CAAC,EACzD;oBAAC;oBAAK;oBAAU,KAAK,SAAS,CAAC;wBAAE;oBAAM;iBAAG;YAElD;YACA,MAAM,OAAO,KAAK,CAAC;YACnB,OAAO;QACX,EAAE,OAAO,OAAO;YACZ,MAAM,OAAO,KAAK,CAAC;YACnB,MAAM;QACV,SAAU;YACN,OAAO,OAAO;QAClB;IACJ;IAEA,MAAM,aAAa,KAAa,EAAE;QAC9B,yEAAyE;QACzE,uEAAuE;QACvE,MAAM,MAAM,CAAC;;;;;;;;IAQjB,CAAC;QAEG,MAAM,SAAS,MAAM,sHAAI,CAAC,KAAK,CAAC,KAAK;YAAC;SAAM;QAC5C,MAAM,MAAM,OAAO,IAAI,CAAC,EAAE;QAE1B,OAAO;YACH,QAAQ;YACR,QAAQ,IAAI,SAAS,IAAI,IAAI,KAAK,IAAI,IAAI,KAAK,GAAG,IAAI,cAAc;YACpE,UAAU;gBACN,YAAY,SAAS,IAAI,KAAK;gBAC9B,WAAW,SAAS,IAAI,SAAS;gBACjC,QAAQ,SAAS,IAAI,MAAM;gBAC3B,aAAa,SAAS,IAAI,WAAW;YACzC;QACJ;IACJ;AACJ;AAEO,MAAM,iBAAiB,IAAI"}},
    {"offset": {"line": 156, "column": 0}, "map": {"version":3,"sources":["file:///D:/search-engine-spec/app/api/v1/admin/crawl/route.ts"],"sourcesContent":["import { NextRequest, NextResponse } from 'next/server';\r\nimport { crawlerService } from '@/lib/services/crawler-service';\r\n\r\nexport async function POST(req: NextRequest) {\r\n    try {\r\n        const body = await req.json();\r\n        const { urls, priority } = body;\r\n\r\n        if (!urls || !Array.isArray(urls)) {\r\n            return NextResponse.json({ error: 'Invalid URLs provided' }, { status: 400 });\r\n        }\r\n\r\n        const jobId = await crawlerService.queueUrls(urls, priority === 'high' ? 10 : 5);\r\n\r\n        return NextResponse.json({\r\n            job_id: jobId,\r\n            status: 'queued',\r\n            urls_queued: urls.length,\r\n            estimated_time_minutes: urls.length * 0.5, // Mock estimation\r\n        });\r\n    } catch (error) {\r\n        console.error('Crawl API error:', error);\r\n        return NextResponse.json({ error: 'Internal server error' }, { status: 500 });\r\n    }\r\n}\r\n"],"names":[],"mappings":";;;;AAAA;AACA;;;AAEO,eAAe,KAAK,GAAgB;IACvC,IAAI;QACA,MAAM,OAAO,MAAM,IAAI,IAAI;QAC3B,MAAM,EAAE,IAAI,EAAE,QAAQ,EAAE,GAAG;QAE3B,IAAI,CAAC,QAAQ,CAAC,MAAM,OAAO,CAAC,OAAO;YAC/B,OAAO,gJAAY,CAAC,IAAI,CAAC;gBAAE,OAAO;YAAwB,GAAG;gBAAE,QAAQ;YAAI;QAC/E;QAEA,MAAM,QAAQ,MAAM,yJAAc,CAAC,SAAS,CAAC,MAAM,aAAa,SAAS,KAAK;QAE9E,OAAO,gJAAY,CAAC,IAAI,CAAC;YACrB,QAAQ;YACR,QAAQ;YACR,aAAa,KAAK,MAAM;YACxB,wBAAwB,KAAK,MAAM,GAAG;QAC1C;IACJ,EAAE,OAAO,OAAO;QACZ,QAAQ,KAAK,CAAC,oBAAoB;QAClC,OAAO,gJAAY,CAAC,IAAI,CAAC;YAAE,OAAO;QAAwB,GAAG;YAAE,QAAQ;QAAI;IAC/E;AACJ"}}]
}